{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speech_command_rec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JeEd0TF43Dw"
      },
      "source": [
        "# Speech Command Recognition\n",
        "\n",
        "We will build an audio classifier network for recognizing trigger commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RBpSiH55lFL"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri3Ne4so5vVx",
        "outputId": "b8cadf23-ee0a-4b61-e595-ee36029bfa5d"
      },
      "source": [
        "!pip install pydub torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchviz"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: torch==1.7.0+cu101 in /usr/local/lib/python3.7/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision==0.8.1+cu101 in /usr/local/lib/python3.7/dist-packages (0.8.1+cu101)\n",
            "Requirement already satisfied: torchaudio==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cu101) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cu101) (0.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1+cu101) (7.1.2)\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.7.0+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (0.6)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4151 sha256=c6f06df7f81cbbd82274b0cb777663c43d023f5483f859b5a752a09cc81ee04e\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVY_rz944ufi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from torchviz import make_dot\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "#select device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7ewyqD7544_"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We use torchaudio to download and represent the dataset. Here we use\n",
        "[SpeechCommands](https://arxiv.org/abs/1804.03209), which is a\n",
        "datasets of 35 commands spoken by different people. The dataset\n",
        "``SPEECHCOMMANDS`` is a ``torch.utils.data.Dataset`` version of the\n",
        "dataset. In this dataset, all audio files are about 1 second long (and\n",
        "so about 16000 time frames long)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVFmnwpK7EeQ"
      },
      "source": [
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "\n",
        "#helper class to split data into training, validation and test sets\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(\"./\", download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as fileobj:\n",
        "                return [os.path.join(self._path, line.strip()) for line in fileobj]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
        "            excludes = set(excludes)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]\n",
        "\n",
        "\n",
        "# Create training and testing split of the data. We do not use validation in this tutorial.\n",
        "train_set = SubsetSC(\"training\")\n",
        "valid_set = SubsetSC(\"validation\")\n",
        "test_set = SubsetSC(\"testing\")\n",
        "\n",
        "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0] #sample for visualizations"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHTITwdJ9iUh"
      },
      "source": [
        "Find all labels in dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNgdx7F09hjh"
      },
      "source": [
        "labels = sorted(list(set(datapoint[2] for datapoint in train_set)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r8czZ2q8rOy"
      },
      "source": [
        "Sample input waveform:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "SlR546oz7165",
        "outputId": "9804fd4f-347e-4686-d9d1-bb1414fdacf7"
      },
      "source": [
        "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "\n",
        "plt.plot(waveform.t().numpy());"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of waveform: torch.Size([1, 16000])\n",
            "Sample rate of waveform: 16000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU1f3/8deHJUR2kEVkC5sIiIoE3BUUFKEu7de22Nbd2tbaTatFsUpdKuq3v1r7tVVqXaqtS7VWFNQKomhVNmVfQ0DZCfsSyHp+f8xNuJnMlswkk5l5Px+PPHLn3HPvfHKT3M+cc+4915xziIhIZmuU7ABERCT5lAxERETJQERElAxERAQlAxERAZokO4Da6NChg8vJyUl2GCIiKWXBggU7nHMdQ61LyWSQk5PD/Pnzkx2GiEhKMbMvw61TN5GIiCQmGZjZGDNbZWZ5ZjYhxPrfm9lC72u1me3xrSvzrZuaiHhERKRm4u4mMrPGwOPAaGAjMM/MpjrnllfUcc79wlf/J8AQ3y4OOedOjjcOERGpvUS0DIYDec65fOdcMfAScGmE+lcALybgfUVEJEESkQy6Aht8rzd6ZdWYWU+gF/C+rzjbzOab2Wdmdlm4NzGzG7168wsKChIQtoiIVKjvAeTxwKvOuTJfWU/nXC7wHeBRM+sTakPn3BTnXK5zLrdjx5BXRomISC0lIhlsArr7XnfzykIZT1AXkXNuk/c9H/iAquMJIiJSDxKRDOYB/cysl5llETjhV7sqyMyOB9oBn/rK2plZM2+5A3AmsDx4WxGR+lawv4h3lm5Ndhj1Ju5k4JwrBW4G3gVWAK8455aZ2b1mdomv6njgJVf1AQoDgPlmtgiYBUz2X4UkIpIsVz89lx++sICDRaXJDqVeJOQOZOfcdGB6UNndQa8nhdjuE2BwImIQEUmkDbsKASjLkAeA6Q5kEZEIMiQXKBmIiIRkyQ6gfikZiIiIkoGISETqJhIRkUyhZCAiEkmGjB0oGYiIRKJuIhGRzJUhDYJKSgYiIhG4DGkaKBmIiIRgllltAyUDERGfv326nh0HipIdRr1LyNxEIiLpYPW2/dz9xrKMmq20gloGIiKe4tJyAPYUliQ5kvqnZCAiEsQ/ZKyJ6kREMox/zDjDxo+VDEREgrlMaQ74KBmIiESQKWlByUBEJIQM6yVSMhARqWC+FJApLYIKSgYiIqJkICJSocrVRMkLIykSkgzMbIyZrTKzPDObEGL9NWZWYGYLva8bfOuuNrM13tfViYhHRCRRMuXKorinozCzxsDjwGhgIzDPzKY655YHVX3ZOXdz0LbtgXuAXAJddAu8bXfHG5eISDw0UV3NDQfynHP5zrli4CXg0hi3vRB4zzm3y0sA7wFjEhCTiEhEew+V8Ot/L+VwSRmPz8rjvreCP79mlkRMVNcV2OB7vRE4NUS9/zGzc4DVwC+ccxvCbNs11JuY2Y3AjQA9evRIQNgikolueWUhfTq2pGB/Ec9/9iXHdW7JI++uAuDyod2AzJmCwq++BpDfBHKccycS+PT/XE134Jyb4pzLdc7lduzYMeEBikhm+Nfnm3jk3VWUlQfO+OW+E3+G9QxVkYhksAno7nvdzSur5Jzb6ZyrmCD8KWBorNuKiNSFSCf+4Keb9b/rbcY8OruOI0quRCSDeUA/M+tlZlnAeGCqv4KZdfG9vARY4S2/C1xgZu3MrB1wgVcmIlIrh0vK+Pyr2K9B8V8tZCEuKHVAUWk5K7fuT0R4DVbcycA5VwrcTOAkvgJ4xTm3zMzuNbNLvGo/NbNlZrYI+ClwjbftLuA+AgllHnCvVyYiUit3vr6Eb/zpEzbuLqws21tYwuOz8igvj30wINN6jBLypDPn3HRgelDZ3b7lO4A7wmz7NPB0IuIQEVm2aR8AB4pKK8t+/cZSpi7azKBjW9d4f5kymKw7kEUkLflP4ge9xFBS5u8Sirxtpg0mKxmISFqpOIm7GK8S8n/w99fLlBZBBSUDEUkLyzbvZf/hyM8urjJYHCFD7C4sTlhcqSIhYwYiIsk27rGPGdqzXZi1Nevz2XGgmA4ts+IPKoWoZSAiaWPBl4mf1iz4noODRaVpOXmdkoGIpKXgk3igLES9sOf16q2JTXsOMeied3n2k/XxhNYgKRmISNoLNagcsl6U/Xy58yAA7y7bGn9QDYzGDEQk5YXrtvl07U6OaZMd+TLSqDuvbVSpRclARNJKxVVCzsEVf/kMgAsGdo5x29jeIw2HDNRNJCISq1BzF6ULJQMRySD++wwi16xYX5aOzYAQlAxEJOVVuds4xPpIA8jhxhsqprC4983MeAKakoGIZKTQXT5HygqLywB4e2n1K4fSsa2gZCAiEqN0nrxOyUBEUl60T+oVrYCo9dL4ZB+NkoGIpKxlm/fyyrwNVcpqOmupXzpOMxEr3WcgIilr3GMfA/A/Q7tVlkWcrjpKgvi/9/MSFVrKUctARNJerAli3voYJ7pLwwaEkoGIpLz66t5J5yEFJQMRSUuhZy1Nw4/0CZKQZGBmY8xslZnlmdmEEOtvMbPlZrbYzGaaWU/fujIzW+h9TU1EPCKSuSqvHHLVy6rWE7+4B5DNrDHwODAa2AjMM7Opzjn/bXtfALnOuUIz+xHwMPBtb90h59zJ8cYhIuJ3sLi0WlnIO5B9rYVMvpooES2D4UCecy7fOVcMvARc6q/gnJvlnCv0Xn4GdENEJEFCncLn5O+KuE089xQ4HDsOFPHsf9elTQJJRDLoCvgv9N3olYVzPfC273W2mc03s8/M7LJwG5nZjV69+QUFBfFFLCIpbd76XZSWlYdcF/IkX3HvQYhV/nP55r2HI76v+Xb+0xe/YNKby1m97UCUaFNDvd5nYGbfA3KBc33FPZ1zm8ysN/C+mS1xzq0N3tY5NwWYApCbm5seqVhEamzhhj1884lPuWlEn4j1tvpO7IkeH3AO9hSWAFASJimlmkS0DDYB3X2vu3llVZjZKGAicIlzrqii3Dm3yfueD3wADElATCKSpgr2B04fq7buD7m+rDzwWfHl+RuqrfN36VQ+BCfRAaaoRCSDeUA/M+tlZlnAeKDKVUFmNgR4kkAi2O4rb2dmzbzlDsCZQGbMFysitdLI+5i/tuBI94y/qydUF76F6DuqKKlNl386zmEUdzeRc67UzG4G3gUaA08755aZ2b3AfOfcVOARoCXwT++X8pVz7hJgAPCkmZUTSEyTg65CEhGpouJEvH5nYcj1jUJ8xP1oTWCccW3BQd+OAt9qc+9BmowZV5GQMQPn3HRgelDZ3b7lUWG2+wQYnIgYRCQz1ObRkxX9+4/NXFNt3cPvrIr9vdOwRVBBdyCLSEppCCfkNGwYKBmISGoJ1f/v7+rZsb84tv3UooUxc0VgyHNrlEtQU5GSgYiklGin8K37YjtR16aF8dRH+QBs2nOo5hs3cEoGIpISxjw6m9cWbKRRgvqJZq3cHr1SBlEyEJGUsHLrfm7956KQn+jfWrSlVvuTI5QMRKTBOlxSxk9f/IKNu0NfRlph6ea99RJPOg4cV9BjL0WkwfpgVQFTF22m0DcDaahOogOHq89QWhfKfTcYLN+yD4DpS7ZwQtc29fL+dUktAxFpsCq6hCqmmAAoC3HH1z8XbKyXeELdbLZkU/20SuqakoGINFgVrQBfLuChd1YmJZZ0p2QgIg1WxT0F/u6ZpZv2JSuckNJlagolAxFpsL7cGZhL6KM1O5IcSXgf5zXc2GpCyUBEGqz7p61IdggZQ8lARESUDERERMlARBqg/YdLKCotS3YYMdu05xCT315Z5UlqqUY3nYlIgzN40n8Y0KV1ssOI2c3/+JwvvtrD107skrI3oKllICIN0ootDesS0khKysqB1L7MVMlARESUDERERMlARCRupWWh+4c+WlPAvPW76jma2tEAsohInCqejeCCJrm+8q9zAVg/eVy9x1RTCWkZmNkYM1tlZnlmNiHE+mZm9rK3fo6Z5fjW3eGVrzKzCxMRj4iknsMlZfxldj6HS1LnktJg1z07P+TlpTkTplVOrdFQxZ0MzKwx8DhwETAQuMLMBgZVux7Y7ZzrC/weeMjbdiAwHhgEjAH+5O1PRDLMH2au4YHpK5j8durOSrrjQBHP/Hd9yHXnPvIBs1cX1G9ANZCIlsFwIM85l++cKwZeAi4NqnMp8Jy3/CpwvgWmI7wUeMk5V+ScWwfkefsTkQyz71AJUPXZBano73O+ZE7+TnYdLK62buK/lwDwztKtfLp2Z32HFlEixgy6Aht8rzcCp4ar45wrNbO9wNFe+WdB23YN9SZmdiNwI0CPHj0SELaINCQVD7J5/rMvkxtInNYWHOTbUz6jf+dW1daVB25H4IcvLAAa1lhCylxN5Jyb4pzLdc7lduzYMdnhiEiCWcgHWqauVdv2VysrKStvsGMiiUgGm4DuvtfdvLKQdcysCdAG2BnjtiKSAUorPjanse37izj+1+9Uvt627zC7Dhaz/3BJEqMKSEQymAf0M7NeZpZFYEB4alCdqcDV3vLlwPsuMOQ+FRjvXW3UC+gHzE1ATEnnnKO0LP3/uGO1YVchNzw3j50HiiLWW7NtPx824EG2ZHriw7XkTJhGcWl6/l0Vl6b2WEFtXPjobE657z0GT/pPskOJPxk450qBm4F3gRXAK865ZWZ2r5ld4lX7K3C0meUBtwATvG2XAa8Ay4F3gB875xpmGypGZeWOnQeK+OU/F9N34tsMe2BGskOqkcMlZRwqTuyv4EBRKWc/PIsZK7Yz9P4ZXPfsPADueWMpF/3hoyp1R/9+Nlc/PZdT7nsv6n5fnvcVOROmcbCoNKHx1tasldsTfuz8nvhwLQA/emEBG3cXxv1pcm3BAaYu2pyI0KpZtnlvjQeCSzLww9OewiO/w1ADzvUpIWMGzrnpzrnjnHN9nHMPeGV3O+emesuHnXPfdM71dc4Nd87l+7Z9wNuuv3Pu7UTEU1NrCw5w7TNzeXneV7Xafv76XUyauozlm/fR587pDL1/Bq99vhGAgv1FFOwv4sHpKygtK+fzr3bz/b/NZ9bK7ZXb//XjdTw+K6/KPvO27+ernYVs3nMooSeYPYXFPPnh2mrXQheVllFaVs6pv53J4EnvcvcbS8mZMI2SsnJyJkyj/121/9XkbT9Q5fX7K7fz3vJtPPfpl6zYso+cCdPYvv9wlTq7DhYzJz/y1RZPfBj4M/pwdQGHissoKStn855DrA7RVxtOWbnjmf+uq5wueW3BAdbvOMjlf/6ELXsPRdz2b5+u5+mP1wGwbsdBrn12HvdMXRrze4dy31vLyZkwjXveWMqVf51TZV3FiWPmyu2c9dAsRv+/2ZXrDhWXcfXTc6tdy15SVs6tryxiwK/fodx3ci4pK+f8333IT1/8AoDycpewq1v+s2wr4x77mD53Tq8s23+4hAmvLWZvYfgElonJwO/ZT9azfd/h6BXriKXi/Nu5ublu/vz5tdr2py9+wdRFm+ne/ig+uv08IHBDSIUZt5xDh5bNaNs8K+Z9+reP5OKTjuVN3yexWb8cwZc7D3LNM/Mqyx7/zikM7tqGcx6ZVWXb3h1akL/jID84tzedWmVzXOeWbNh1iDtfX8Ky31xIi2ZNcM5RXFbOhNeW8KMRfTguxNUMfe6cTlm546UbT+O03kdX+RmyGjeiOOgf8qRubVi0cS8A9106iD/MzGPGLedEPT4zlm/j1N7taZXdlDn5O/n2lM8i1r/2zBzOP74z3ws6AX7+69FVWglPfG8oxx/TihH/+0HE/XVu3Yw5d46KWGfDrkLOfjhwnLOaNOLOi45n0pvLK9dfctKx3HZhf7q3b45zjt2FJbRvEfi5v/XEp8z1phlY9psLWbRhD995KhB7rFeIfLnzIC/P28Ato4/j2mfnhX3O7y8vOI5lm/fx9tKt1dZddXpPWjRrwp8/WFtZdtOIPtw+5ngA7vr3El74LPAh59zjOvLcdcMpK3dVTtRXDO9B/84tmfTmcp66KpdRAzsDgST74pyv+PP3TsHMOFhUyttLt/LsJ+vo2vYonrwyl72FJdz6z4X89uuD6dQ6m7eXbOFHf/+8ct+zbxvJk7PX8u8vNnGwuIzhOe155Yenh/w5b3huHjNWbA+5LlMcf0wr3vn5OZWvdx8sZt3Og5zSo11C9m9mC5xzuSHXZVIyGDzpXfYfrtql0KdjC9YWhL4z8NlrhzGif6cqZXsKi7n3zeXcc8kg7ntrOa8u2FjjOBLtXzedQevsplz8x485FHSlwjPXDuOEY9tU6646uXtbnrxyKK9/sYkfnNObXndMJ1Y9j27Oh7eNDLt+wZe7+J8/f8rZ/Trw/PWn8qcP8nj4nVU1+6ESYO1vx9K4kbFl7yGmzM6vvBnoo9tH0qFlMwbc/U7kHXjO7teB/p1b8dTH63jjx2fy2+krmLPuyHwzp/c+mk99rZjxw7oz+X9OjLjPlVv3MebRjyLWicc7Pz+b449pzXm/+4D8oL/vY1pnszXoE2jPo5vz5c5CJo4dQPf2R/HDF46c0Gf9cgTd2h1Fv4lVW4cL7x7NyfceSdR3jj2e306PfsNYuGR51dNzG/RNWfXl/VvP5eI/fswz1w7n1/9eyqpt+5n+07N58O0VnNm3A1ed3pPmWbW7K0DJwBPrJ3i/Gbecy+GSssoHVjw6YzWPzljDz0f149EZa2q8v3QR7h/aOVclsay4d0zlSfej20dWfhKvD3eNG8C4E7tw+oPvVynPObo5d4wdwA+eX1DjfZ7YrQ2LvZZSJP7jU17u2F1YTNvmWTQymLNuF+OjtJQiGXdiF6Yt3hK13jVn5PDsJ+tr/T5+x3VuyeptB6JXjEH+b8fSqNGRy0g37Cqk9VFN+cHz8/ksPzUmdUumF64/lbP6dajVtpGSgSaqi2LU//sQOPLP/Y7XVA+VCDq1asZvvz6YG/52JFGd1bcDH+eFbv6ng0PFZdz+2mImXTyQo1s2q9Yy8X/67t6+eUz7fOsnZ/Hqgo0xnchaZTep0tqb9tOzGPfYxwDcP20F909bUW2b9TsLa5UIgJgSAQS6gFplN602EP6T8/ryx/fzwmwVcNuF/SnYXxTy519w1yjat8iiZVYTXp6/ofrGPolKBEDCEgHAvsMlVboZKz4gnNQtNZ8QVt8GHVs3T4BLmZvOEiGrceDHvfK0ntXWLZ50AQB9O7UMuW3OhGnkTJhWOTthsDP7Hs3ciaMYNbAzL994Gucd34nFky7g6WuG8cy1w6rUvfikY3nwG4OrlL31k7P483dPqXx93Zm9WD95HHPuPD/2HzAGQ3u249KTj42p7v99Z0jYdRNfX0LOhGlM/PcS3ly0maH3z+DleV+x91DoAcKR/QM3Ci6ZdAFPXjmU9ZPH8cy1w1h492iuGN69St0TurZh0iWDOKVH28qyUQM68+L3T6tS7x83nMriey6gV4cWfP/sXnz8q5Ecf0zN/1HWPTi2cvkbQ0LeAB/W0S1Cj52c+8gH3Pvmsmrl4RLBFcO784/vn8qiey7gxyP7MumSQdVaXxMuOp6jWzbDzPj+Ob2rrKv4206kj38VviswmlX3j6lW9rtvngTAr15bXFmWX3AkyQR34UpoR2XVzfRtGdVNlLf9AH+YuYZHLj+Rpo0b0cjgzMnvs3nvYdZPHsfijXvo07Elg+55N+Z9Pnz5iSzcsIf7Lz2hStM3mL+LquKfPFLZw5efyLdyu1cpe/2mM7j8iU8pK3e0b5HFroPFtDmqKSVl5RT6rjjq1KoZg45tzaxVVftfe7RvzuzbR7J932GG/3ZmtRjXPTiWTXsOcdZDs6rEBDD2Dx+xfMs+7rl4IL/xDbLGatE9F9DmqKZR6znnMG9egt/9ZxV/fD+PmbeeS5+OgSRdcSzWPTi2sl6wf32+kVteWVSt/Nozc6pNInbDWb2462sDmbVyO4+9v4bXfnhG5e8xb/uBypZhsI9uH8nMFdsYc0IXTntwJjeN6MOI/p341pOfRv0Z/dZPHsfhkjKym4b+B1+1dT/3vbWcJ68cSotmRxryzjn+MHMNFww8ht4dW9CkIuaCA9XGIlplN2Hmrefy2Mw1bNx9iA98fxfzJo7i7aVbWLV1P3+fExho/uG5fTi2bTZXnZ5TrWv12WuHcVK3tgyJcOnvlaf15L7LTmDCa4vZd7iEBV/uZsqVucxcsY3HorSKJLpIf/vRaMwggsMlZRQWl1VeJQKByw0NuOTxj1m6KfJzWGO9cqTin8p/NcXs1QVc9fTcyoFOgAt/P5tV2/bz+HdOYdyJXYBAn+qXOws5q18Hdh4oYtnmfZxz3JEpOcrLHfPW7+K/eTv4xejjqvyh7DpYXNlV4Y9154EiFm/ay75DJfzspYX07tiC928dAQSa8YdLyujUKrvKcdpTWMLKrfuqXP0UzuiBnXlv+TYA3v7Z2bV6uHlpWTnrdhykn++qqNmrC5izbie3XXh8xG2vf3YeFwzqTIeWzTijT4fKT1P973qbIu+mrVm/HEGvDi0i7mfDrkKKy8ppnd20chB+5X1jqpy8N+4upEubo2jcyHhj4SZ+9tLCmH6+0QM785erQv5fxuWk3/ynSgtt7sTzq/wuS8rKue7Zefx8VD+G9mxfWX7WQ++z+2Axy+498ql+7rpdtG/RlJVb93PCsW3I8Y7XxNeX8M8FG1l135jKv7eX533Fr15bwpoHLqJpiJZKUWkZ/e+KbdBewotnPiMlgzi88NmX3PXv0NeOt2zWhKW/ie0RDIdLynhl/gbGD+tBVpPwTfq9hSU888k6bh7ZlyYJavq/PO8rPl27k0fHh+/2idXCDXu47PH/Rq039eYzObFb26j16tuBolLWbj/AZ/k7+cG5fWq07R9mrGFoz3ZRB+/yCw5w3u+OtCiCW31ZTRqx+v6Lahh57Oat38V3n5rDlCuHMqR7O9o0j94ig8DJ2jnCtlISod/E6ZSEeSqYxKaukkFGjRnUxpgTjgEClxc+c02g77+RBa7Nfv762Gfbzm7amKtOz4mYCADaNG/Kz0cdl7BEAPDtYT0SkgggcEnqaO869AqDu1Yd+Duha+sGmQggkMBP6t62xokA4Gej+sV0FUfvji257sxeYdfX9XQSw3Las/r+ixjRv1PMiQCgWZPGdZoIAJb9ZgxPXjm0Tt+jofvuqbWfdfmei4MfFZM4upooig4tm1XJxP/7zZM4tVf7mK+MSUdPfG8ov3h5Iau37ee2C/tz/oDOrN9xkPunLefR8UNo2Ux/VjNWbKtW9o8bTuU7T83hmjNy6j+gBiKrSSMuHHQM6yeP44uvdvP1P31CVpNGaTvfUih3jB1QOT5TU+OH1d30/WoZ1NDlQ7tldCIAaNzIeOyKIbzz83M4f0CglZDToQVPXT1MicDzqjcu9MaPz6wsO6NvB565dhh3jI083pEpBnRpzQUDO/PmzWclO5R61dzX+vrTd09h3OAuVdaf1K0NocaHv3FK1zq7kgjUMhCpE51aZ4fs2x0ZdEd7Jstu2pgp3gD6x78aSX7BQa56Oi0mLQ7rxG5tKq9Wa9zIGDu4C2MHd+Hk2fk8MD1wT0xOhxb87bpTyd9xgF0Hi7n+ucD46Dn96vY5LkoGIpJ03do1p1u75nRs1YyC/ZGnOU8l3xjSlX99ceQRLXeOHQAcmWOrguPIoLoRGDsc4s1HtH7yOPZ4d7DXJXUTiUiDkQ7POrvRd0NgRbdO+xZZrLxvTOXkkGNOOKbyMl2Ab+V258RubejUqhk/Ob9ftX3WdSIAtQxERBLq5vP6MmV2YHr1ijG0k7u3jXilVtvmWUxN8tiJkoGINBjpcAdC6+ymLL/3Qh5+ZxU/G9WPsYO7hJ3mpiFRMhARSZAh3nxazbOaMOmSQQCc1L1h3nMTTGMGIiIJkspjHkoGItJgpOrJ9NFvn5zsEOKmZCAiDUbFmMGto49Lahw1VfFYynOPS937SDRmICINxsOXn8gj76yifcu6v5Qykbq2O4q5d55Ph5bNkh1KrcXVMjCz9mb2npmt8b5Xe2qzmZ1sZp+a2TIzW2xm3/ate9bM1pnZQu8r9dtaIlJrI/t3YvrPzqZRLefrTxYjcNd5pGeaNHTxdhNNAGY65/oBM73XwQqBq5xzg4AxwKNm5h9ev805d7L3FdtE8CIiDUiK5a6Q4k0GlwLPecvPAZcFV3DOrXbOrfGWNwPbgbqdZENEUlqkD9gndK39M4Dr6oN7bZ881pDEmww6O+e2eMtbgc6RKpvZcCALWOsrfsDrPvq9mYXtcDOzG81svpnNLygoCFdNRNLA14d0C7uuSaPan7bS4aRdV6IeVTObYWZLQ3xd6q/nAo9MC3sDoZl1AZ4HrnXOVUxefgdwPDAMaA/8Ktz2zrkpzrlc51xux45qWIiks0gPgdL5vG5EvZrIOTcq3Doz22ZmXZxzW7yT/fYw9VoD04CJzrnPfPuuaFUUmdkzwC9rFL2IZITLh3bj1QUbkx1GWou3m2gqcLW3fDXwRnAFM8sCXgf+5px7NWhdF++7ERhvCP2wYRHJaN8e1r1yOVGPbW+drSvr/eJNBpOB0Wa2BhjlvcbMcs3sKa/Ot4BzgGtCXEL6dzNbAiwBOgD3xxmPiKSJiu6guXeez7Cc9tXK49UqO/bnQ2eCuFKjc24ncH6I8vnADd7yC8ALYbY/L573F5H09fldoykqLadT6+wq5RoyqBtqJ4lIg9SuRd3ehayB6Ko0N5GIZAz/+T/WZBD8wPp0pWQgIimlvu8V6NgqdecbqgklAxHJGP45j0yjD1UoGYhISjkqwrOEo/Kd/zVmUJWSgYiklHieJ2xhlmN104g+1cq6tj2q1vE0JEoGIiIxyg7RKnnrJ2clIZLEUzIQkYwRb9dQqM3r+hLY+qJkICJpabjvruUK/kHjigfRnNW3Q8T9ZMrYgpKBiGSMViHmI7rn4oGVy985tUfE7RM0LVKDpGQgImkvu2ngVPf0NcOqrdMzDgKUDEQkYxwb5cqfUDOi+ruW0jltKBmISIZL586f2CkZiEjGiP7JPnJi8K89vffRcUbTsGjWUhFJKS5RT7epdCRF1GTXT18zjB0HihIcS/KoZSAiGSOR9xkcldWY7u2bx7fDBkTJQE8KXPoAAA3cSURBVERS3snd28ax9ZHmQPCDdDKJkoGIpLwubWI7iUebqbRFVhyT4KU4JQMRSSmh7guIbxghnS8YjZ2SgYiklFADyDGPBdTivJ8p96TFlQzMrL2ZvWdma7zv7cLUKzOzhd7XVF95LzObY2Z5ZvaymaXHjE8iknDPXDuM1350RuXrPh1b1HgfoU/sus8A4m8ZTABmOuf6ATO916Eccs6d7H1d4it/CPi9c64vsBu4Ps54RCRNjezfiaE9j3ze/N5pPSNvoFZAjcSbDC4FnvOWnwMui3VDC3T8nQe8WpvtRSSz1ea8HXqbyHvKlPwQbzLo7Jzb4i1vBTqHqZdtZvPN7DMzqzjhHw3scc6Veq83Al3jjEdEMoS/cyfUJ3pvhmoaN4r9dJ7w+9lSSNQ7kM1sBnBMiFUT/S+cc87Mwh3Kns65TWbWG3jfzJYAe2sSqJndCNwI0KNH5GlmRURye7bns/xdVcpqM0PpCV3bJCqkBi1qy8A5N8o5d0KIrzeAbWbWBcD7vj3MPjZ53/OBD4AhwE6grZlVJKRuwKYIcUxxzuU653I7duxYgx9RRNJRtNP6TSP7xFQvmsuGVO+w6BfHc5gbqni7iaYCV3vLVwNvBFcws3Zm1sxb7gCcCSx3gevDZgGXR9peRKQ2GoVoBURLDLE2HC4YFK5HPHXFmwwmA6PNbA0wynuNmeWa2VNenQHAfDNbRODkP9k5t9xb9yvgFjPLIzCG8Nc44xGRDDSwS+uE7u8bIVoDftHuZE5Fcc1a6pzbCZwfonw+cIO3/AkwOMz2+cDweGIQkcwSqt//5O4hb3GqVDEwXGXTECOcFfWObhn5lieXhvcm6A5kEUkpvxh1HFcM7863h1W/kOTMvpGfMRDrJ/pwA83pfB+CnmcgIimlTfOmPPiNE2u0TciTeIQTe7hnJqTzpadqGYiIeGL95J+OYwZKBiKSNvyf3P0n9pCf6OP4lK8xAxGRDJbOYwZKBiKSNkKdrM1iHzOINiagMQMRkRQQ6mTtXOK7iTRmICLSAIVsEYQ4YYdrOURaH4rGDEREGqBI3TfRBpJj7foZ0qNtWo8Z6D4DEUl7Nb3PINjnvx5N86zG/GV2fsJiamiUDEQk5YU62fu7cmIdMwjXSmjfIv2fyKtuIhHJGNHGDGqzfbpQMhCRlBfqE32oAeRoYwbpfLKPRslARNJS1Ct+anHi130GIiINmP8TfW7PqtNZ+1sI0aawzmQaQBaRtPLCDadyoKi08rXDRbxJLOocRmHqphslAxFJeRUPoxnQpTXZTRuT3bQxRaVl1erF2s0T7nkG6UzJQERS3vHHtOb1m87ghK5tqq2LNnVE6EHlzHuegZKBiKSFIT0iP/oSonfzZGCDoJIGkEUkLTVtFDi9/WxUv4j1NGYQoJaBiKSlRo2M9ZPHAfD0x+soKi2OedtMHDOIq2VgZu3N7D0zW+N9r9ZOM7ORZrbQ93XYzC7z1j1rZut8606OJx4RkVBe/dEZ3DVuANlNG8e1n0aNAkmiURomi3hbBhOAmc65yWY2wXv9K38F59ws4GQIJA8gD/iPr8ptzrlX44xDRCSsXh1acMPZvWOuH24A+Zozcti85xA/OLdPokJrMOJNBpcCI7zl54APCEoGQS4H3nbOFcb5viIifHrHeRSXlidsf9E+8DfPasL9lw1O2Ps1JPEOIHd2zm3xlrcCnaPUHw+8GFT2gJktNrPfm1mzcBua2Y1mNt/M5hcUFMQRsoikiy5tjqLn0S1qvN3NI/uGLE/nS0ejidoyMLMZwDEhVk30v3DOOTMLeyjNrAswGHjXV3wHgSSSBUwh0Kq4N9T2zrkpXh1yc3Mz+FcmIvGoGFTeebCY/B0HaZVd/TSYiQPIUZOBc25UuHVmts3Mujjntngn++0RdvUt4HXnXIlv3xWtiiIzewb4ZYxxi4jE5TeXDOKGs3vRqVV2skNpEOLtJpoKXO0tXw28EaHuFQR1EXkJBAuk4cuApXHGIyISk6wmjejTsWXIdeEGkNNZvMlgMjDazNYAo7zXmFmumT1VUcnMcoDuwIdB2//dzJYAS4AOwP1xxiMiUmsZ2DtUKa6riZxzO4HzQ5TPB27wvV4PdA1R77x43l9EJJEysEFQSXcgi0jGa5HVmFN8z0HQALKISAZadu8YAJ78cG2SI0keTVQnIhJEA8giIpKRlAxERIJk4piBkoGIiCgZiIiIkoGISDUaQBYRkYykZCAiEkQDyCIikpGUDERERMlARCSYBpBFRASA7KaZdXrURHUiIkHMjNm3j2TH/uJkh1JvlAxERELo1Co7ox6JmVntIBERCUnJQETE07RxI+975t1noG4iERHPd07twbZ9h7lpRN9kh1LvlAxERDzZTRtzx9gByQ4jKdRNJCIi8SUDM/ummS0zs3Izy41Qb4yZrTKzPDOb4CvvZWZzvPKXzSwrnnhERKR24m0ZLAW+AcwOV8HMGgOPAxcBA4ErzGygt/oh4PfOub7AbuD6OOMREZFaiCsZOOdWOOdWRak2HMhzzuU754qBl4BLLTAt4HnAq16954DL4olHRERqpz7GDLoCG3yvN3plRwN7nHOlQeUhmdmNZjbfzOYXFBTUWbAiIpko6tVEZjYDOCbEqonOuTcSH1JozrkpwBSA3NzczJtFSkSkDkVNBs65UXG+xyagu+91N69sJ9DWzJp4rYOKchERqWf10U00D+jnXTmUBYwHprrAHLGzgMu9elcD9dbSEBGRIyyeebvN7OvAH4GOwB5goXPuQjM7FnjKOTfWqzcWeBRoDDztnHvAK+9NYEC5PfAF8D3nXFEM71sAfFnLsDsAO2q5bV1SXDWjuGpGcdVMusbV0znXMdSKuJJBKjKz+c65sPdEJIviqhnFVTOKq2YyMS7dgSwiIkoGIiKSmclgSrIDCENx1YziqhnFVTMZF1fGjRmIiEh1mdgyEBGRIEoGIiKSWckg3FTadfRe3c1slpkt96b5/plX3t7M3jOzNd73dl65mdljXmyLzewU376u9uqvMbOrExRfYzP7wsze8l6HnE7czJp5r/O89Tm+fdzhla8yswsTEFNbM3vVzFaa2QozO70hHC8z+4X3O1xqZi+aWXayjpeZPW1m281sqa8sYcfIzIaa2RJvm8fMLKbnP4aJ6xHvd7nYzF43s7bRjkW4/9Fwx7s2cfnW3Wpmzsw6NITj5ZX/xDtmy8zs4Xo9Xs65jPgicMPbWqA3kAUsAgbW4ft1AU7xllsBqwlM4f0wMMErnwA85C2PBd4GDDgNmOOVtwfyve/tvOV2CYjvFuAfwFve61eA8d7yE8CPvOWbgCe85fHAy97yQO8YNgN6ece2cZwxPQfc4C1nAW2TfbwITJ64DjjKd5yuSdbxAs4BTgGW+soSdoyAuV5d87a9KI64LgCaeMsP+eIKeSyI8D8a7njXJi6vvDvwLoGbVzs0kOM1EpgBNPNed6rP41UnJ8KG+AWcDrzre30HcEc9vv8bwGhgFdDFK+sCrPKWnwSu8NVf5a2/AnjSV16lXi1j6QbMJDCF+FveH/IO3z9u5bHy/mFO95abePUs+Pj569UypjYETroWVJ7U48WRWXfbez//W8CFyTxeQE7QSSQhx8hbt9JXXqVeTeMKWvd14O/ecshjQZj/0Uh/n7WNi8DU+ScB6zmSDJJ6vAicwEeFqFcvxyuTuonCTaVd57yugiHAHKCzc26Lt2or0DlKfHUR96PA7UC59zrSdOKV7++t3+vVT3RcvYAC4BkLdF89ZWYtSPLxcs5tAv4X+ArYQuDnX0Dyj5dfoo5RV2+5LmK8jsAn59rEVaPp7qMxs0uBTc65RUGrkn28jgPO9rp3PjSzYbWMq1bHK5OSQVKYWUvgNeDnzrl9/nUukLbr9dpeM/sasN05t6A+3zcGTQg0m//snBsCHCTQ5VEpScerHXApgWR1LNACGFOfMdREMo5RNGY2ESgF/t4AYmkO3AncnexYQmhCoAV6GnAb8EqsYxCJkEnJINxU2nXGzJoSSAR/d879yyveZmZdvPVdgO1R4kt03GcCl5jZegKTBJ4H/AFvOvEQ71H5/t76NgSmH090XBuBjc65Od7rVwkkh2Qfr1HAOudcgXOuBPgXgWOY7OPll6hjtMlbTliMZnYN8DXgu16iqk1cldPdJyCuPgQS+yLvf6Ab8LmZHVOLuBJ9vDYC/3IBcwm03DvUIq7aHa/a9Fmm4heBrJtP4A+hYrBlUB2+nwF/Ax4NKn+EqoN9D3vL46g6eDXXK29PoC+9nfe1DmifoBhHcGQA+Z9UHXC6yVv+MVUHRF/xlgdRdVArn/gHkD8C+nvLk7xjldTjBZwKLAOae+/1HPCTZB4vqvc1J+wYUX1AdGwccY0BlgMdg+qFPBZE+B8Nd7xrE1fQuvUcGTNI9vH6IXCvt3wcgS4gq6/jVScnwob6ReBqgdUERuAn1vF7nUWgub4YWOh9jSXQnzcTWEPgyoGKPyoDHvdiWwLk+vZ1HZDnfV2bwBhHcCQZ9Pb+sPO8P6SKKxqyvdd53vrevu0nevGuIsarKKLEczIw3ztm//b+8ZJ+vIDfACuBpcDz3j9lUo4X8CKBsYsSAp8kr0/kMQJyvZ9zLfB/BA3o1zCuPAIntIq//yeiHQvC/I+GO961iSto/XqOJINkH68s4AVvf58D59Xn8dJ0FCIiklFjBiIiEoaSgYiIKBmIiIiSgYiIoGQgIiIoGYiICEoGIiIC/H9j2kmLHKdOWAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7otaxcZ8YeK"
      },
      "source": [
        "### Data Processing\n",
        "\n",
        "For data transformations, we can downsample the audio for faster procesing without losing much classification power. We choose to not do any further processing since SpeechCommands uses a single channel for audio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMp82iqE9Ryv"
      },
      "source": [
        "Word encoding and lookup helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRMQ3Y9K9EpJ"
      },
      "source": [
        "def label_to_index(word):\n",
        "    # Return the position of the word in labels\n",
        "    return torch.tensor(labels.index(word))\n",
        "\n",
        "\n",
        "def index_to_label(index):\n",
        "    # Return the word corresponding to the index in labels\n",
        "    # This is the inverse of label_to_index\n",
        "    return labels[index]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4PP9OWN9-_J"
      },
      "source": [
        "Collate data to turn list of data points into 2 batched tensors; Apply padding so tensors have consistent shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccE4cYJL-Lrx"
      },
      "source": [
        "def pad_sequence(batch):\n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch = [item.t() for item in batch]\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "    return batch.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    # A data tuple has the form:\n",
        "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
        "\n",
        "    tensors, targets = [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for waveform, _, label, *_ in batch:\n",
        "        tensors += [waveform]\n",
        "        targets += [label_to_index(label)]\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    tensors = pad_sequence(tensors)\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return tensors, targets\n",
        "\n",
        "#constants\n",
        "batch_size = 256\n",
        "if device == \"cuda\":\n",
        "    num_workers = 1\n",
        "    pin_memory = True\n",
        "else:\n",
        "    num_workers = 0\n",
        "    pin_memory = False\n",
        "\n",
        "#create dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8HO-fiH-rd5"
      },
      "source": [
        "### Network Definition\n",
        "\n",
        "We will build a CNN following the M5 network architecture explained in paper [Very Deep Convolutional Neural Networks for Raw Waveforms](https://arxiv.org/pdf/1610.00087.pdf).\n",
        "\n",
        "Note an important aspect for models processing raw audio is the receptive field of their first layers' filers. Our model's first layer has length 80, and we process audio samples at 8kHz, so we get a receptive field near 10ms which is fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2urAjmPBxjc",
        "outputId": "095b6b2d-b01c-4a2a-dbc1-00687faebc14"
      },
      "source": [
        "class M5(nn.Module):\n",
        "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
        "        self.pool1 = nn.MaxPool1d(4)\n",
        "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
        "        self.pool2 = nn.MaxPool1d(4)\n",
        "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool3 = nn.MaxPool1d(4)\n",
        "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool4 = nn.MaxPool1d(4)\n",
        "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.bn2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(self.bn3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(self.bn4(x))\n",
        "        x = self.pool4(x)\n",
        "        x = F.avg_pool1d(x, x.shape[-1])\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=2)\n",
        "\n",
        "\n",
        "model = M5(n_input=transformed.shape[0], n_output=len(labels))\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M5(\n",
            "  (conv1): Conv1d(1, 32, kernel_size=(80,), stride=(16,))\n",
            "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
            "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
            "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
            "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=64, out_features=35, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUR0WYdsDLyo"
      },
      "source": [
        "### Training\n",
        "\n",
        "#### Loss Metric\n",
        "We will use negative log likelihood loss.\n",
        "\n",
        "#### Optimization\n",
        "Following the paper [Very Deep Convolutional Neural Networks for Raw Waveforms](https://arxiv.org/pdf/1610.00087.pdf), we will use an Adam optimizer with weight decay set to 0.0001, and learning rate scheduler starting at 0.01, decreasing to 0.001 over 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqMzBvpbD0Qm"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPkc7QJNEIG1"
      },
      "source": [
        "Training function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgIkAVfNEVS8"
      },
      "source": [
        "def train(model, epoch, log_interval):\n",
        "  train_loss = []\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "      # apply transform and model on whole batch directly on device\n",
        "      data = transform(data)\n",
        "      output = model(data)\n",
        "\n",
        "      # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
        "      loss = F.nll_loss(output.squeeze(), target)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print training stats\n",
        "      if batch_idx % log_interval == 0:\n",
        "          print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "          \n",
        "      # record loss\n",
        "      train_loss.append(loss.item())\n",
        "\n",
        "  #return final loss\n",
        "  return train_loss[-1]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv7VDmVNE7Pu"
      },
      "source": [
        "Testing function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sepj1RlE8bR"
      },
      "source": [
        "def number_of_correct(pred, target):\n",
        "    return pred.squeeze().eq(target).sum().item()\n",
        "\n",
        "def get_likely_index(tensor):\n",
        "    return tensor.argmax(dim=-1)\n",
        "\n",
        "def test(model, epoch, dataldr, name):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "      for data, target in dataldr:\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "\n",
        "          # apply transform and model on whole batch directly on device\n",
        "          data = transform(data)\n",
        "          output = model(data)\n",
        "\n",
        "          pred = get_likely_index(output)\n",
        "          correct += number_of_correct(pred, target)\n",
        "\n",
        "          loss += F.nll_loss(output.squeeze(), target)\n",
        "\n",
        "      print(f\"{name} -  Epoch: {epoch}\\tAccuracy: {correct}/{len(dataldr.dataset)} ({100. * correct / len(dataldr.dataset):.0f}%)\")\n",
        "    \n",
        "    return 100 * correct / len(dataldr.dataset), loss / len(dataldr.dataset)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzMMMiACFTpc"
      },
      "source": [
        "Train and test network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlO7K3JAFU65"
      },
      "source": [
        "#constants\n",
        "log_interval = 20\n",
        "n_epoch = 21\n",
        "\n",
        "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "# The transform needs to live on the same device as the model and the data.\n",
        "transform = transform.to(device)\n",
        "with tqdm(total=n_epoch) as pbar:\n",
        "    for epoch in range(1, n_epoch + 1):\n",
        "      #training\n",
        "      train(model, epoch, log_interval)\n",
        "      print()\n",
        "      train_acc, train_loss = test(model, epoch, train_loader, \"Train Set\")\n",
        "      #validating\n",
        "      valid_acc, valid_loss = test(model, epoch, valid_loader, \"Valid Set\")\n",
        "      print()\n",
        "      #record\n",
        "      train_losses.append(train_loss)\n",
        "      train_accs.append(train_acc)\n",
        "      valid_losses.append(valid_loss)\n",
        "      valid_accs.append(valid_acc)\n",
        "      #scheduler update\n",
        "      scheduler.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvurwBgak1yd"
      },
      "source": [
        "Visualize results of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbwjNFxLk1KJ"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(train_accs)\n",
        "plt.plot(valid_accs)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel = 'accuracy'\n",
        "plt.xlabel = 'epoch'\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(train_losses)\n",
        "plt.plot(valid_losses)\n",
        "plt.title('model loss')\n",
        "plt.ylabel = 'loss'\n",
        "plt.xlabel = 'epoch'\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtJwsMoKG7-Q"
      },
      "source": [
        "### Model Evaluation\n",
        "\n",
        "Testing set results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja-czmDQML3m",
        "outputId": "96f8fc0e-023a-4e57-d47b-bdcfc1cb6db0"
      },
      "source": [
        "test(model, epoch, valid_loader, \"Test Set\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set -  Epoch: 21\tAccuracy: 8709/9981 (87%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(87.25578599338743, tensor(0.0017, device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AN35zOhMbMl"
      },
      "source": [
        "Prediction helper function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odrqw1i3Lfq1"
      },
      "source": [
        "def predict(tensor):\n",
        "    # Use the model to predict the label of the waveform\n",
        "    tensor = tensor.to(device)\n",
        "    tensor = transform(tensor)\n",
        "    tensor = model(tensor.unsqueeze(0))\n",
        "    tensor = get_likely_index(tensor)\n",
        "    tensor = index_to_label(tensor.squeeze())\n",
        "    return tensor"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIip4uz0Mg7z"
      },
      "source": [
        "#### Open Testing\n",
        "\n",
        "We can create our own test by recording a sample while executing the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "gEm1BeHINDFI",
        "outputId": "616e58d9-2d38-43db-abae-1aa7c24d5bfe"
      },
      "source": [
        "def record(seconds=1):\n",
        "\n",
        "    from google.colab import output as colab_output\n",
        "    from base64 import b64decode\n",
        "    from io import BytesIO\n",
        "    from pydub import AudioSegment\n",
        "\n",
        "    RECORD = (\n",
        "        b\"const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\\n\"\n",
        "        b\"const b2text = blob => new Promise(resolve => {\\n\"\n",
        "        b\"  const reader = new FileReader()\\n\"\n",
        "        b\"  reader.onloadend = e => resolve(e.srcElement.result)\\n\"\n",
        "        b\"  reader.readAsDataURL(blob)\\n\"\n",
        "        b\"})\\n\"\n",
        "        b\"var record = time => new Promise(async resolve => {\\n\"\n",
        "        b\"  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\\n\"\n",
        "        b\"  recorder = new MediaRecorder(stream)\\n\"\n",
        "        b\"  chunks = []\\n\"\n",
        "        b\"  recorder.ondataavailable = e => chunks.push(e.data)\\n\"\n",
        "        b\"  recorder.start()\\n\"\n",
        "        b\"  await sleep(time)\\n\"\n",
        "        b\"  recorder.onstop = async ()=>{\\n\"\n",
        "        b\"    blob = new Blob(chunks)\\n\"\n",
        "        b\"    text = await b2text(blob)\\n\"\n",
        "        b\"    resolve(text)\\n\"\n",
        "        b\"  }\\n\"\n",
        "        b\"  recorder.stop()\\n\"\n",
        "        b\"})\"\n",
        "    )\n",
        "    RECORD = RECORD.decode(\"ascii\")\n",
        "\n",
        "    print(f\"Recording started for {seconds} seconds.\")\n",
        "    display(ipd.Javascript(RECORD))\n",
        "    s = colab_output.eval_js(\"record(%d)\" % (seconds * 1000))\n",
        "    print(\"Recording ended.\")\n",
        "    b = b64decode(s.split(\",\")[1])\n",
        "\n",
        "    fileformat = \"wav\"\n",
        "    filename = f\"_audio.{fileformat}\"\n",
        "    AudioSegment.from_file(BytesIO(b)).export(filename, format=fileformat)\n",
        "    return torchaudio.load(filename)\n",
        "\n",
        "\n",
        "# Detect whether notebook runs in google colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    waveform, sample_rate = record()\n",
        "    print(f\"Predicted: {predict(waveform)}.\")\n",
        "    ipd.Audio(waveform.numpy(), rate=sample_rate)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recording started for 1 seconds.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recording ended.\n",
            "Predicted: wow.\n"
          ]
        }
      ]
    }
  ]
}